<div id="describe-text">
	
	<p>PhD in Physics, Secondary in Data Science</p>
  
	<p>Research Interests: <strong>Machine Learning: Interpretability and Explainibility, Visualization of AI</strong></p>
	
	<p align="left"> Current Projects: </p>
	<p align="left"> Interpreting Deep Neural Networks: Probing modularity and specialization in Neural Networks. Specialized Neurons emerge in Vision and Large Language Netowrks. Can we modify the current architecture to induce Specialization in an Interpretable way?   </p>
	<img src="{{ '/assets/img/branched.png' | prepend: site.baseurl }}" id="branched" width="500">
	<p align="left"> Explainable AI: What is common between subsets of data within a clustered region in a t-SNE or UMAP projection of high-dimensional 			data? We're building the Next-gen Visualization tools that Visualization Clusters of Big data and Langauge, and explain what causes 			these clusters to form!</p>
		  
	<img src="{{ '/assets/img/clusters.png' | prepend: site.baseurl }}" id="clusters" width="700">
	<p align="left"> Data Visualization: Always curious about visualizing interesting datasets! </p>
</div>

---
layout: page
title: Résumé
subtitle: Interested? Get in touch!
---
<img src="{{ '/assets/img/pic.jpg' | prepend: site.baseurl }}" id="about-img">
